# llm_chain.py
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from dotenv import load_dotenv
import os
load_dotenv()
os.environ["OPENAI_API_KEY"]=os.getenv("OPENAI_API_KEY")
# Initialize the OpenAI GPT-4 model via LangChain
# (Make sure OPENAI_API_KEY is set in environment or pass it here)
llm = ChatOpenAI(model_name="gpt-4", temperature=0, request_timeout=30)

# Prepare a system message with schema information and instructions for the AI
schema_info = """
Database Schema (AdventureWorks2019 - simplified):
- Table [Person].[Person]: Columns = BusinessEntityID, FirstName, LastName, EmailPromotion, ...
- Table [Sales].[SalesOrderHeader]: Columns = SalesOrderID, OrderDate, TotalDue, CustomerID, ...
- Table [Sales].[SalesOrderDetail]: Columns = SalesOrderID, ProductID, OrderQty, LineTotal, ...
- Table [Production].[Product]: Columns = ProductID, Name, ProductNumber, ListPrice, ...
... (include other relevant tables and columns as needed) ...
"""
# The schema_info string should list tables and important columns to help the LLM.

system_prompt = (
    "You are an expert SQL assistant for the AdventureWorks2019 database. "
    "You will be given a user question and the database schema. "
    "Generate a single SQL SELECT query (and no explanatory text) that answers the question. "
    "The database may have schemas like Person, Sales, Production; include schema qualifiers in table names (e.g., Person.Person). "
    "Only produce SQL, without additional commentary."
    + schema_info
)
system_message = SystemMessage(content=system_prompt)

def get_sql_for_question(user_question, history):
    """
    Given the user question (string) and the conversation history (list of messages),
    return an SQL query string generated by GPT-4.
    """
    # Build the message list for context: start with system message (instructions + schema)
    messages = [system_message]
    # Include conversation history to maintain context
    # We'll add only the textual content of past interactions.
    # We include the user's questions and assistant answers (but exclude any SQL from answers to avoid confusion).
    for msg in history:
        role = msg.get("role")
        content = msg.get("content", "")
        if role == "user":
            messages.append(HumanMessage(content=content))
        elif role == "assistant":
            # If the assistant content was an error or a result, we include it as AI message
            # to give context for follow-up questions.
            # (We might exclude the raw SQL from the history to the LLM to avoid repeating it.)
            messages.append(AIMessage(content=content))
    # Add the latest user question as the final human message
    messages.append(HumanMessage(content=user_question))
    # Call the LLM to get a response
    response = llm(messages)
    sql_query = response.content.strip()
    return sql_query
